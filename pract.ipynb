{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'king' and 'queen': 0.9493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example word embeddings for \"king\" and \"queen\"\n",
    "embedding_king = np.array([0.2, 0.5, 0.1])\n",
    "embedding_queen = np.array([0.3, 0.4, 0.2])\n",
    "\n",
    "# Reshape the embeddings to be 2D arrays (as required by sklearn's cosine_similarity function)\n",
    "embedding_king_reshaped = embedding_king.reshape(1, -1)\n",
    "embedding_queen_reshaped = embedding_queen.reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(embedding_king_reshaped, embedding_queen_reshaped)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Cosine Similarity between 'king' and 'queen': {cosine_sim[0][0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.38.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (0.23.2)\n",
      "Requirement already satisfied: Pillow in /home/irshad/.local/lib/python3.10/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/irshad/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/irshad/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/irshad/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: wheel in /home/irshad/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence_transformers) (0.44.0)\n",
      "Requirement already satisfied: cmake in /home/irshad/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (3.30.3)\n",
      "Requirement already satisfied: lit in /home/irshad/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (18.1.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/irshad/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/irshad/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/irshad/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/irshad/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/irshad/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/irshad/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/irshad/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/irshad/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/irshad/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/irshad/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/irshad/.local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, sentence_transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.3.5 requires transformers<4.37.0,>=3.4.0, but you have transformers 4.45.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sentence_transformers-3.1.1 tokenizers-0.20.0 transformers-4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/irshad/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/irshad/.local/lib/python3.10/site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/irshad/.local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/irshad/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/irshad/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/irshad/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/irshad/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/irshad/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/irshad/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/irshad/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console-script.py console.exe dejavu dejavu.exe events\n",
      "events.exe execute execute.exe kernel kernel.exe kernelspec kernelspec.exe lab\n",
      "lab.exe labextension labextension.exe labhub labhub.exe migrate migrate.exe\n",
      "nbconvert nbconvert.exe notebook-script.py notebook.exe qtconsole-script.py\n",
      "qtconsole.exe run run.exe script.py server server.exe troubleshoot\n",
      "troubleshoot.exe trust trust.exe\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between the two sentences: 0.8142\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Example text input\n",
    "text1 = \"I love machine learning\"\n",
    "text2 = \"Machine learning is fascinating\"\n",
    "\n",
    "# Generate embeddings for the text inputs\n",
    "embedding1 = model.encode([text1])\n",
    "embedding2 = model.encode([text2])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# Output the similarity score\n",
    "print(f\"Similarity score between the two sentences: {similarity_score[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between the two sentences: 0.8349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example text input\n",
    "text1 = \"I love machine learning\"\n",
    "text2 = \"Machine learning is fascinating\"\n",
    "\n",
    "# Generate embeddings for the text inputs\n",
    "embedding1 = model.encode([text1])\n",
    "embedding2 = model.encode([text2])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# Output the similarity score\n",
    "print(f\"Similarity score between the two sentences: {similarity_score[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.5677\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences to compare\n",
    "sent1 = \"The car is red\"\n",
    "sent2 = \"The vehicle is crimson\"\n",
    "\n",
    "# Generate sentence embeddings\n",
    "embedding1 = model.encode([sent1])\n",
    "embedding2 = model.encode([sent2])\n",
    "\n",
    "# Compute similarity\n",
    "similarity_score = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# Print the similarity score\n",
    "print(f\"Similarity score: {similarity_score[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.5852\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences to compare\n",
    "sent1 = \"The car is red\"\n",
    "sent2 = \"The vehicle is crimson\"\n",
    "\n",
    "# Generate sentence embeddings\n",
    "embedding1 = model.encode([sent1])\n",
    "embedding2 = model.encode([sent2])\n",
    "\n",
    "# Compute similarity\n",
    "similarity_score = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# Print the similarity score\n",
    "print(f\"Similarity score: {similarity_score[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"The cat is on the mat\", \"The feline is on the mat\", 0.9),  # High similarity\n",
    "    (\"The sky is blue\", \"The grass is green\", 0.2),  # Low similarity\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_paper= {\n",
    "    \"part_a\": [\n",
    "      {\"question_number\": 1, \"question\": \"What is machine learning?\", \"marks\": 2},\n",
    "      {\"question_number\": 2, \"question\": \"Explain overfitting in neural networks.\", \"marks\": 2},\n",
    "      {\"question_number\": 3, \"question\": \"Define supervised learning.\", \"marks\": 2},\n",
    "      {\"question_number\": 4, \"question\": \"What is gradient descent?\", \"marks\": 2},\n",
    "      {\"question_number\": 5, \"question\": \"Explain the difference between AI and ML.\", \"marks\": 2}\n",
    "    ],\n",
    "    \"part_b\": [\n",
    "      {\"question_number\": 1, \"question\": \"Explain different types of activation functions.\", \"marks\": 5},\n",
    "      {\"question_number\": 2, \"question\": \"What are the applications of convolutional neural networks?\", \"marks\": 5}\n",
    "    ],\n",
    "    \"part_c\": [\n",
    "      {\"question_number\": 1, \"question\": \"Explain the working of backpropagation in neural networks with an example.\", \"marks\": 10}\n",
    "    ]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key= [(\n",
    "\"Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.\"\n",
    "\"Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.\"\n",
    "\"Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.\"\n",
    "\"Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.\"\n",
    "\"Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.\"\n",
    "      ),\n",
    "    \n",
    "    (\"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"\n",
    "\"Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.\"\n",
    "    ),\n",
    "    \n",
    "    (\"Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.\"\n",
    "    )\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [\n",
    "    {\n",
    "      \"student_id\": \"CSE2024001\",\n",
    "              \"answers\": [\n",
    "                  \"Machine learning is a part of AI that learns from data.\",\n",
    "                  \"Overfitting means the model performs well on training data but poorly on unseen data.\",\n",
    "                  \"Supervised learning is when the machine learns from labeled examples.\",\n",
    "                  \"\",\n",
    "                  \"AI is broader than ML, where ML is a subset focused on learning from data.\"\n",
    "                ],\n",
    "                 {\n",
    "                  \"1\": \"Activation functions include ReLU, Tanh, and Sigmoid. They help neurons decide.\",\n",
    "                  \"2\": \"CNNs are applied in images and video processing.\"\n",
    "                },\n",
    "                \"part_c\": {\n",
    "                  \"1\": \"Backpropagation is used in neural networks to reduce error by adjusting weights backward based on gradients.\"\n",
    "                }\n",
    "      }\n",
    "    },\n",
    "\n",
    "    {\n",
    "      \"student_id\": \"AIDS2024005\",\n",
    "      \"name\": \"Jane Smith\",\n",
    "      \"department\": \"AIDS\",\n",
    "              \"answers\": {\n",
    "                \"part_a\": {\n",
    "                  \"1\": \"Machine learning allows systems to learn from experience.\",\n",
    "                  \"2\": \"Overfitting is when the model performs well on training data but not on test data.\",\n",
    "                  \"3\": \"Supervised learning involves labeled data.\",\n",
    "                  \"4\": \"Gradient descent is used to optimize the model.\",\n",
    "                  \"5\": \"AI is general, ML is specific to learning from data.\"\n",
    "                },\n",
    "                \"part_b\": {\n",
    "                  \"1\": \"ReLU, Sigmoid, and Tanh are activation functions in neural networks.\",\n",
    "                  \"2\": \"CNNs are used in image processing and facial recognition.\"\n",
    "                },\n",
    "                \"part_c\": {\n",
    "                  \"1\": \"Backpropagation updates the weights of a neural network to reduce error.\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"student_id\": \"ECE2024010\",\n",
    "      \"name\": \"Alice Brown\",\n",
    "      \"department\": \"ECE\",\n",
    "      \"answers\": {\n",
    "        \"part_a\": {\n",
    "          \"1\": \"\",\n",
    "          \"2\": \"Overfitting happens when the model fits the training data too well.\",\n",
    "          \"3\": \"\",\n",
    "          \"4\": \"Gradient descent helps in finding the minimum cost.\",\n",
    "          \"5\": \"\"\n",
    "        },\n",
    "        \"part_b\": {\n",
    "          \"1\": \"\",\n",
    "          \"2\": \"\"\n",
    "        },\n",
    "        \"part_c\": {\n",
    "          \"1\": \"Backpropagation updates the network weights based on error feedback.\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "students= [{\n",
    "                \"student_id\": \"S001\",\n",
    "                    \"answers\": [\n",
    "                            (\"Machine learning is a technology that allows computers to learn from data and make decisions based on that data.\",\n",
    "                             \"Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.\",\n",
    "                             \"Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.\",\n",
    "                             \"Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.\",\n",
    "                             \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\",\n",
    "                            ),\n",
    "\n",
    "                            (\"Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.\",\n",
    "                             \"Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.\"\n",
    "                             ),\n",
    "\n",
    "                            (\"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"\n",
    "                             )\n",
    "                            ]\n",
    "            },\n",
    "            {\n",
    "                    \"student_id\": \"S002\",\n",
    "                    \"answers\": [\n",
    "                                (\"Machine learning is when machines learn from data.\",\n",
    "                                \"Overfitting is when a model is too complex and learns the noise.\",\n",
    "                                \"Supervised learning uses labeled data for training.\",\n",
    "                                \"Gradient descent is a way to minimize the loss function.\",\n",
    "                                \"AI is the big idea, while ML is a part of AI.\"\n",
    "                                ),\n",
    "                                \n",
    "                                (\"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\",\n",
    "                                 \"CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.\"\n",
    "                                ),\n",
    "\n",
    "                                (\"Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.\"\n",
    "                                )   \n",
    "                        ]\n",
    "                    \n",
    "            },\n",
    "                \n",
    "            {    \"student_id\": \"S003\",\n",
    "                    \"answers\": [(\n",
    "                                \"Machine learning is a process by which computers analyze data and learn from it.\",\n",
    "                                \"Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.\",\n",
    "                                \"Supervised learning involves training a model on a labeled dataset.\",\n",
    "                                \"Gradient descent is an iterative optimization algorithm used for minimizing a function.\",\n",
    "                                \"AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.\"\n",
    "                                 ),\n",
    "\n",
    "                                (\n",
    "                                \"Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.\",\n",
    "                                \"Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.\"\n",
    "                                ),    \n",
    "                                \n",
    "                                (\"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"\n",
    "                                )\n",
    "                                ]\n",
    "            },\n",
    "            {                      \n",
    "                \"student_id\": \"S004\",\n",
    "                    \"answers\":\n",
    "                                [( \"Machine learning is when computers are trained to learn from data.\",\n",
    "                                    \"Overfitting is a problem where the model learns the training data too well.\",\n",
    "                                    \"Supervised learning is when a model is trained on labeled data.\",\n",
    "                                    \"Gradient descent is a method to reduce error in models.\",\n",
    "                                    \"AI is the concept while ML is a method of achieving AI.\"\n",
    "                                ),\n",
    "                                \n",
    "                                (\n",
    "                                    \"Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.\",\n",
    "                                    \"CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.\"\n",
    "                                ),\n",
    "                                (\"Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.\"\n",
    "                                )]\n",
    "            }\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_number': 1,\n",
       "  'answer': 'Machine learning is when computers are trained to learn from data.'},\n",
       " {'question_number': 2,\n",
       "  'answer': 'Overfitting is a problem where the model learns the training data too well.'},\n",
       " {'question_number': 3,\n",
       "  'answer': 'Supervised learning is when a model is trained on labeled data.'},\n",
       " {'question_number': 4,\n",
       "  'answer': 'Gradient descent is a method to reduce error in models.'},\n",
       " {'question_number': 5,\n",
       "  'answer': 'AI is the concept while ML is a method of achieving AI.'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_info['question_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 3\n",
      "KeyError for question number: 4\n",
      "KeyError for question number: 5\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 3\n",
      "KeyError for question number: 4\n",
      "KeyError for question number: 5\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 3\n",
      "KeyError for question number: 4\n",
      "KeyError for question number: 5\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 3\n",
      "KeyError for question number: 4\n",
      "KeyError for question number: 5\n",
      "KeyError for question number: 1\n",
      "KeyError for question number: 2\n",
      "KeyError for question number: 1\n",
      "  roll_number           name              department  \\\n",
      "0        S001  Alice Johnson        Computer Science   \n",
      "1        S002      Bob Smith  Electrical Engineering   \n",
      "2        S003  Charlie Brown  Information Technology   \n",
      "3        S004      David Lee    Software Engineering   \n",
      "\n",
      "                                           marks  total_part_a  total_part_b  \\\n",
      "0  {'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0}             0             0   \n",
      "1  {'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0}             0             0   \n",
      "2  {'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0}             0             0   \n",
      "3  {'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0}             0             0   \n",
      "\n",
      "   total_part_c  total_marks  \n",
      "0             0            0  \n",
      "1             0            0  \n",
      "2             0            0  \n",
      "3             0            0  \n"
     ]
    }
   ],
   "source": [
    "def encode_answer(answer):\n",
    "    \"\"\"Encodes a student's answer using the model.\"\"\"\n",
    "    return model.encode(answer)\n",
    "\n",
    "def calculate_similarity(student_answer, answer_key_answer):\n",
    "    \"\"\"Calculates cosine similarity between student's answer and answer key.\"\"\"\n",
    "    student_embedding = encode_answer(student_answer)\n",
    "    answer_key_embedding = encode_answer(answer_key_answer)\n",
    "    similarity = cosine_similarity([student_embedding], [answer_key_embedding])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def process_answers(part_answers, answer_key, part_weight):\n",
    "    \"\"\"Processes answers for a specific part and calculates marks.\"\"\"\n",
    "    total_marks = 0\n",
    "    marks = {}\n",
    "    \n",
    "    for answer_info in part_answers:\n",
    "        q_num = answer_info[\"question_number\"] - 1  # Adjust for zero-based index\n",
    "        answer = answer_info[\"answer\"]\n",
    "        \n",
    "        if answer:  # Check if answer is not empty\n",
    "            try:\n",
    "                similarity = calculate_similarity(answer, answer_key[q_num])\n",
    "                marks_for_question = round(similarity * part_weight, 2)  # Assign marks based on similarity\n",
    "                marks[f\"q{answer_info['question_number']}\"] = marks_for_question\n",
    "                total_marks += marks_for_question\n",
    "            except KeyError:\n",
    "                print(f\"KeyError for question number: {answer_info['question_number']}\")\n",
    "                marks[f\"q{answer_info['question_number']}\"] = 0\n",
    "        else:\n",
    "            marks[f\"q{answer_info['question_number']}\"] = 0  # No marks for empty answers\n",
    "\n",
    "    return total_marks, marks\n",
    "\n",
    "def grade_student(student, answer_key):\n",
    "    \"\"\"Grades a single student's answers across all parts.\"\"\"\n",
    "    student_results = {\n",
    "        \"student_id\": student[\"student_id\"],\n",
    "        \"name\": student[\"name\"],\n",
    "        \"department\": student[\"department\"],\n",
    "        \"marks\": {}\n",
    "    }\n",
    "    \n",
    "    # Process Part A\n",
    "    total_part_a, part_a_marks = process_answers(student[\"answers\"][\"part_a\"], answer_key[\"part_a\"], 2)\n",
    "    student_results[\"marks\"].update(part_a_marks)\n",
    "    student_results[\"total_part_a\"] = total_part_a\n",
    "    \n",
    "    # Process Part B\n",
    "    total_part_b, part_b_marks = process_answers(student[\"answers\"][\"part_b\"], answer_key[\"part_b\"], 5)\n",
    "    student_results[\"marks\"].update(part_b_marks)\n",
    "    student_results[\"total_part_b\"] = total_part_b\n",
    "\n",
    "    # Process Part C\n",
    "    total_part_c, part_c_marks = process_answers(student[\"answers\"][\"part_c\"], answer_key[\"part_c\"], 10)\n",
    "    student_results[\"marks\"].update(part_c_marks)\n",
    "    student_results[\"total_part_c\"] = total_part_c\n",
    "    \n",
    "    # Calculate total marks\n",
    "    student_results[\"total_marks\"] = total_part_a + total_part_b + total_part_c\n",
    "    return student_results\n",
    "\n",
    "def grade_students(students, answer_key):\n",
    "    \"\"\"Grades all students and compiles results into a DataFrame.\"\"\"\n",
    "    results = []\n",
    "    for student in students:\n",
    "        student_results = grade_student(student, answer_key)\n",
    "        results.append(student_results)\n",
    "\n",
    "    # Create a DataFrame from results\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Sort students by total marks in descending order\n",
    "    df_results = df_results.sort_values(by=\"total_marks\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Example usage\n",
    "df_results = grade_students(students, answer_key)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(student_answer, answer_key_answer):\n",
    "    \"\"\"Calculates cosine similarity between student's answer and answer key.\"\"\"\n",
    "    student_embedding = encode_answer(student_answer)\n",
    "    answer_key_embedding = encode_answer(answer_key_answer)\n",
    "    \n",
    "    # Debug: Print embeddings\n",
    "    print(\"Student Embedding:\", student_embedding)\n",
    "    print(\"Answer Key Embedding:\", answer_key_embedding)\n",
    "    \n",
    "    similarity = cosine_similarity([student_embedding], [answer_key_embedding])[0][0]\n",
    "    \n",
    "    # Debug: Print similarity\n",
    "    print(\"Similarity:\", similarity)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def process_answers(part_answers, answer_key, part_weight):\n",
    "    \"\"\"Processes answers for a specific part and calculates marks.\"\"\"\n",
    "    total_marks = 0\n",
    "    marks = {}\n",
    "    \n",
    "    for answer_info in part_answers:\n",
    "        q_num = answer_info[\"question_number\"] - 1  # Adjust for zero-based index\n",
    "        answer = answer_info[\"answer\"]\n",
    "        \n",
    "        # Debug: Print the current question number and answer\n",
    "        print(f\"Processing Part Answer - Question: {answer_info['question_number']}, Answer: {answer}\")\n",
    "        \n",
    "        if answer:  # Check if answer is not empty\n",
    "            try:\n",
    "                similarity = calculate_similarity(answer, answer_key[q_num])\n",
    "                marks_for_question = round(similarity * part_weight, 2)  # Assign marks based on similarity\n",
    "                marks[f\"q{answer_info['question_number']}\"] = marks_for_question\n",
    "                total_marks += marks_for_question\n",
    "                \n",
    "                # Debug: Print marks for the question\n",
    "                print(f\"Marks for Question {answer_info['question_number']}: {marks_for_question}\")\n",
    "                \n",
    "            except KeyError:\n",
    "                print(f\"KeyError for question number: {answer_info['question_number']}\")\n",
    "                marks[f\"q{answer_info['question_number']}\"] = 0\n",
    "        else:\n",
    "            marks[f\"q{answer_info['question_number']}\"] = 0  # No marks for empty answers\n",
    "\n",
    "    return total_marks, marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Part Answer - Question: 1, Answer: Machine learning is a technology that allows computers to learn from data and make decisions based on that data.\n",
      "KeyError for question number: 1\n",
      "Processing Part Answer - Question: 2, Answer: Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.\n",
      "KeyError for question number: 2\n",
      "Processing Part Answer - Question: 3, Answer: Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.\n",
      "KeyError for question number: 3\n",
      "Processing Part Answer - Question: 4, Answer: Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.\n",
      "KeyError for question number: 4\n",
      "Processing Part Answer - Question: 5, Answer: AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_results \u001b[38;5;241m=\u001b[39m \u001b[43mgrade_students\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 70\u001b[0m, in \u001b[0;36mgrade_students\u001b[0;34m(students, answer_key)\u001b[0m\n\u001b[1;32m     68\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m student \u001b[38;5;129;01min\u001b[39;00m students:\n\u001b[0;32m---> 70\u001b[0m     student_results \u001b[38;5;241m=\u001b[39m \u001b[43mgrade_student\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(student_results)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[54], line 48\u001b[0m, in \u001b[0;36mgrade_student\u001b[0;34m(student, answer_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m student_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroll_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: student[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: student[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m: student[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m     45\u001b[0m }\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Process Part A\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m total_part_a, part_a_marks \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpart_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_key\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpart_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m student_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(part_a_marks)\n\u001b[1;32m     50\u001b[0m student_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_part_a\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m total_part_a\n",
      "Cell \u001b[0;32mIn[59], line 31\u001b[0m, in \u001b[0;36mprocess_answers\u001b[0;34m(part_answers, answer_key, part_weight)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer:  \u001b[38;5;66;03m# Check if answer is not empty\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         similarity \u001b[38;5;241m=\u001b[39m calculate_similarity(answer, \u001b[43manswer_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_num\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     32\u001b[0m         marks_for_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(similarity \u001b[38;5;241m*\u001b[39m part_weight, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Assign marks based on similarity\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         marks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m marks_for_question\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_results = grade_students(students, answer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_student(student, answer_key):\n",
    "    \"\"\"Grades a single student's answers across all parts.\"\"\"\n",
    "    student_results = {\n",
    "        \"student_id\": student[\"student_id\"],\n",
    "        \"name\": student[\"name\"],\n",
    "        \"department\": student[\"department\"],\n",
    "        \"marks\": {}\n",
    "    }\n",
    "    \n",
    "    # Process Part A\n",
    "    total_part_a, part_a_marks = process_answers(student[\"answers\"][\"part_a\"], answer_key[\"part_a\"], 2)\n",
    "    student_results[\"marks\"].update(part_a_marks)\n",
    "    student_results[\"total_part_a\"] = total_part_a\n",
    "    \n",
    "    # Process Part B\n",
    "    total_part_b, part_b_marks = process_answers(student[\"answers\"][\"part_b\"], answer_key[\"part_b\"], 5)\n",
    "    student_results[\"marks\"].update(part_b_marks)\n",
    "    student_results[\"total_part_b\"] = total_part_b\n",
    "\n",
    "    # Process Part C\n",
    "    total_part_c, part_c_marks = process_answers(student[\"answers\"][\"part_c\"], answer_key[\"part_c\"], 10)\n",
    "    student_results[\"marks\"].update(part_c_marks)\n",
    "    student_results[\"total_part_c\"] = total_part_c\n",
    "    \n",
    "    # Calculate total marks\n",
    "    student_results[\"total_marks\"] = total_part_a + total_part_b + total_part_c\n",
    "    return student_results\n",
    "\n",
    "def process_answers(part_answers, answer_key, part_weight):\n",
    "    \"\"\"Processes answers for a specific part and calculates marks.\"\"\"\n",
    "    total_marks = 0\n",
    "    marks = {}\n",
    "    \n",
    "    for answer_info in part_answers:\n",
    "        q_num = answer_info[\"question_number\"] - 1  # Adjust for zero-based index\n",
    "        answer = answer_info[\"answer\"]\n",
    "        \n",
    "        # Debug: Print the current question number and answer\n",
    "        print(f\"Processing Part Answer - Question: {answer_info['question_number']}, Answer: {answer}\")\n",
    "\n",
    "grade_student(student, answer_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"Saved output to {filename}\")\n",
    "\n",
    "def convert_to_embeddings(data):\n",
    "    print(\"Starting embedding conversion...\")\n",
    "    embeddings = model.encode(data)\n",
    "    print(\"Finished embedding conversion.\")\n",
    "    return embeddings\n",
    "\n",
    "def calculate_cosine_similarity(answer_embedding, key_embedding):\n",
    "    print(\"Calculating cosine similarity...\")\n",
    "    similarity = cosine_similarity([answer_embedding], [key_embedding])[0][0]\n",
    "    print(f\"Cosine similarity: {similarity}\")\n",
    "    return similarity\n",
    "\n",
    "def score_from_similarity(similarity, max_marks):\n",
    "    print(\"Converting cosine similarity to score...\")\n",
    "    score = similarity * max_marks  # Simple scaling\n",
    "    print(f\"Score: {score} out of {max_marks}\")\n",
    "    return score\n",
    "\n",
    "def calculate_total_marks(student_scores):\n",
    "    \n",
    "    print(\"Calculating total marks for student...\")\n",
    "    total_marks = sum(student_scores)\n",
    "    print(f\"Total marks: {total_marks}\")\n",
    "    return total_marks\n",
    "\n",
    "def process_student(student, answer_key, save_intermediate):\n",
    "    scores = []\n",
    "\n",
    "    print(f\" Inside process_student \\n\\nstudent = {student} \\n scores ={scores}\")\n",
    "    for part, questions in answer_key.items():\n",
    "       \n",
    "        print(f\"\\n\\█▓▒▒░░░PART░░░▒▒▓█{part}\\n\\n \\\n",
    "                            questions: {questions} \\n\\n\\\n",
    "                                  from answer_key.items {answer_key.items()} \\n\\n\")\n",
    "        \n",
    "        for question in questions:\n",
    "            print(f\"question: {question}\\n\\n\")\n",
    "            question_number = str(question[\"question_number\"])\n",
    "            print(f\"str (question_number): {question_number}\\n\\n\\n\\n\")\n",
    "            print(\"getting ans from \",student[\"answers\"],\"\\n\\n Printing \",part,\"\\n\\n\",student[\"answers\"][part] ,\"\\n\")\n",
    "            answer = student[\"answers\"][part][0].get(question_number, \"\")\n",
    "            print(f\"str (answer): {answer}\")\n",
    "\n",
    "            if answer:  # Only process if answer is provided\n",
    "                embedding_answer = convert_to_embeddings([answer])[0]\n",
    "                print(\"embedding_answer = \\n \",embedding_answer)\n",
    "                embedding_key = convert_to_embeddings([question[\"answer\"]])[0]\n",
    "                print(\"embedding_key = \\n \",embedding_key)\n",
    "                similarity = calculate_cosine_similarity(embedding_answer, embedding_key)\n",
    "                print(\"similarity=\", similarity)\n",
    "                score = score_from_similarity(similarity, question[\"marks\"])\n",
    "                print(score)\n",
    "                scores.append(score)\n",
    "                print(scores)\n",
    "            else:\n",
    "                scores.append(0)  # No answer results in 0 score\n",
    "\n",
    "            if save_intermediate:\n",
    "                save_output(scores, f'intermediate_scores_{student[\"student_id\"]}.json')\n",
    "\n",
    "    total_marks = calculate_total_marks(scores)\n",
    "    return scores, total_marks\n",
    "\n",
    "def calculate_results(students, answer_key, save_intermediate=False):\n",
    "    results = []\n",
    "    for student in students:\n",
    "        print(f\"✌𝓟𝓻𝓸𝓬𝓮𝓼𝓼𝓲𝓷𝓰 𝓼𝓽𝓾𝓭𝓮𝓷𝓽✌: {student['student_id']}\")\n",
    "        scores, total_marks = process_student(student, answer_key, save_intermediate)\n",
    "        result = {\n",
    "            \"student_id\": student[\"student_id\"],\n",
    "            \"scores\": scores,\n",
    "            \"total_marks\": total_marks\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    final_df = pd.DataFrame(results)\n",
    "    print(\"Final results DataFrame created.\")\n",
    "    if not save_intermediate:\n",
    "        final_df.to_csv('final_results.csv', index=False)\n",
    "        print(\"Saved final results to final_results.csv\")\n",
    "    return final_df\n",
    "\n",
    "# Example usage:\n",
    "# final_results = calculate_results(students, answer_key, save_intermediate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✌𝓟𝓻𝓸𝓬𝓮𝓼𝓼𝓲𝓷𝓰 𝓼𝓽𝓾𝓭𝓮𝓷𝓽✌: S001\n",
      " Inside process_student \n",
      "\n",
      "student = {'student_id': 'S001', 'name': 'Alice Johnson', 'department': 'Computer Science', 'answers': {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]}} \n",
      " scores =[]\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_a\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "question: {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "question: {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}\n",
      "\n",
      "\n",
      "str (question_number): 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "question: {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}\n",
      "\n",
      "\n",
      "str (question_number): 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "question: {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}\n",
      "\n",
      "\n",
      "str (question_number): 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_b\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "question: {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_c\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a technology that allows computers to learn from data and make decisions based on that data.'}, {'question_number': 2, 'answer': 'Overfitting happens when a model learns the details of the training data to an extent that it negatively impacts the performance on new data.'}, {'question_number': 3, 'answer': 'Supervised learning is when we train a model on labeled data, which means that we provide the correct output for every input.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in a model by iteratively adjusting the parameters.'}, {'question_number': 5, 'answer': \"AI is the broader concept of machines being able to carry out tasks in a way that we would consider 'smart', while ML is a specific subset of AI that focuses on the idea that we should really just be able to give machines access to data and let them learn for themselves.\"}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are vital components of neural networks that help to introduce non-linearity into the model, allowing it to learn complex patterns in the data. There are various types of activation functions, including:\\n\\n1. **Sigmoid Function**: This function outputs a value between 0 and 1, making it useful for binary classification. However, it can cause issues during training due to the vanishing gradient problem, where gradients become too small for effective learning.\\n\\n2. **Tanh Function**: This is similar to the sigmoid but outputs values between -1 and 1, which helps to center the data, leading to better convergence.\\n\\n3. **ReLU (Rectified Linear Unit)**: This function outputs the input directly if it is positive; otherwise, it outputs zero. It is the most commonly used activation function in deep learning because it reduces the likelihood of vanishing gradients and speeds up training significantly.\\n\\n4. **Softmax Function**: This function is often used in the output layer of multi-class classification problems. It converts logits into probabilities by ensuring the sum of all probabilities equals 1. \\n\\nChoosing the right activation function is crucial as it can affect the performance and convergence of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have revolutionized how we approach image processing and computer vision. They are designed to automatically and adaptively learn spatial hierarchies of features from input images. Here are some key applications:\\n\\n1. **Image Classification**: CNNs are extensively used for classifying images into categories. For instance, they can distinguish between cats and dogs in pictures.\\n\\n2. **Object Detection**: They can identify and locate objects within an image, such as finding faces in a crowd or detecting cars in traffic.\\n\\n3. **Image Segmentation**: CNNs are applied in segmenting images into meaningful parts, allowing for more granular analysis. For example, segmenting medical images to isolate tumors.\\n\\n4. **Facial Recognition**: Used in security systems and social media, CNNs help to identify individuals in images based on their facial features.\\n\\n5. **Video Analysis**: CNNs can be used in video data for activity recognition, such as identifying specific actions in a surveillance video.\\n\\n6. **Medical Image Analysis**: CNNs assist in diagnosing diseases by analyzing medical images like X-rays or MRIs. They can highlight anomalies, aiding doctors in making informed decisions.\\n\\nIn conclusion, the application of CNNs spans various fields, showcasing their versatility and efficiency in processing visual data.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}]} \n",
      "\n",
      " Printing  part_c \n",
      "\n",
      " [{'question_number': 1, 'answer': \"Backpropagation is a fundamental algorithm used to train neural networks. It works in two main phases: forward propagation and backward propagation.\\n\\n1. **Forward Propagation**: Initially, the input data is fed into the network, and it moves from the input layer through the hidden layers to the output layer. Each neuron applies a weighted sum of its inputs and then passes this through an activation function to produce an output. The outputs are compared against the true labels using a loss function to compute the error.\\n\\n2. **Backward Propagation**: The next step involves calculating the gradient of the loss function with respect to each weight in the network by applying the chain rule. Starting from the output layer, we compute how much each weight contributed to the error. These gradients are then used to update the weights in the direction that minimizes the error. This is often done using an optimization algorithm like gradient descent.\\n\\n**Example**: Consider a simple neural network with one hidden layer. Let's say we have an input (e.g., an image) and we want to predict its class (e.g., cat or dog). During the forward pass, the input data is processed, and the network generates an output. After comparing this output with the true label, we compute the loss. In the backward pass, we calculate the gradients for each weight based on the loss and adjust them accordingly to improve the model's accuracy in subsequent iterations. Through repeated cycles of forward and backward propagation, the model learns to make better predictions, adapting its weights to reduce errors over time.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S001.json\n",
      "Calculating total marks for student...\n",
      "Total marks: 0\n",
      "✌𝓟𝓻𝓸𝓬𝓮𝓼𝓼𝓲𝓷𝓰 𝓼𝓽𝓾𝓭𝓮𝓷𝓽✌: S002\n",
      " Inside process_student \n",
      "\n",
      "student = {'student_id': 'S002', 'name': 'Bob Smith', 'department': 'Electrical Engineering', 'answers': {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]}} \n",
      " scores =[]\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_a\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "question: {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "question: {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}\n",
      "\n",
      "\n",
      "str (question_number): 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "question: {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}\n",
      "\n",
      "\n",
      "str (question_number): 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "question: {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}\n",
      "\n",
      "\n",
      "str (question_number): 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_b\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "question: {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_c\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when machines learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is when a model is too complex and learns the noise.'}, {'question_number': 3, 'answer': 'Supervised learning uses labeled data for training.'}, {'question_number': 4, 'answer': 'Gradient descent is a way to minimize the loss function.'}, {'question_number': 5, 'answer': 'AI is the big idea, while ML is a part of AI.'}], 'part_b': [{'question_number': 1, 'answer': \"There are many types of activation functions in neural networks that help to decide how to process information. One is the sigmoid function, which outputs numbers between 0 and 1. It is good for binary classification but can slow down learning because of the vanishing gradient problem. Another function is tanh, which outputs between -1 and 1. This is better than sigmoid because it centers data around zero.\\n\\nThe ReLU function, which stands for Rectified Linear Unit, is widely used. It outputs the input directly if it's positive; otherwise, it outputs zero. This helps in speeding up learning. Lastly, the softmax function is used in multi-class classification problems. It takes a vector of scores and converts them into probabilities, making it easier to classify multiple classes. In summary, the choice of activation function can greatly affect how well a neural network learns.\"}, {'question_number': 2, 'answer': 'CNNs are used for many things in image processing. They are very useful in image classification where the goal is to identify what an image contains. They can also be used for object detection, meaning they can find where objects are located within an image. Another use is in image segmentation, which is breaking an image into different parts to analyze.\\n\\nFacial recognition is a popular application, enabling security systems to identify people based on their facial features. Moreover, CNNs are useful in analyzing medical images, helping doctors diagnose diseases by highlighting anomalies. In video analysis, CNNs can identify actions or behaviors over time. Overall, CNNs are vital tools for analyzing visual data.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}]} \n",
      "\n",
      " Printing  part_c \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Backpropagation is an algorithm used to train neural networks. It works by first calculating the output of the network, comparing it to the actual target output, and computing the error. Then, it uses this error to update the weights in the network.\\n\\nIn simple terms, the process begins with a forward pass where the input data moves through the network to generate an output. Then, the loss is calculated. After that, in the backward pass, the algorithm computes how much each weight contributed to the loss by calculating gradients.\\n\\nFor instance, in a neural network with input, hidden, and output layers, when an image is processed, the output will be compared to the actual label. The difference is the error. This error is then backpropagated through the layers to adjust the weights to improve accuracy. Over time, through many iterations, the network learns to make better predictions.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S002.json\n",
      "Calculating total marks for student...\n",
      "Total marks: 0\n",
      "✌𝓟𝓻𝓸𝓬𝓮𝓼𝓼𝓲𝓷𝓰 𝓼𝓽𝓾𝓭𝓮𝓷𝓽✌: S003\n",
      " Inside process_student \n",
      "\n",
      "student = {'student_id': 'S003', 'name': 'Charlie Brown', 'department': 'Information Technology', 'answers': {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]}} \n",
      " scores =[]\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_a\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "question: {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "question: {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}\n",
      "\n",
      "\n",
      "str (question_number): 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "question: {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}\n",
      "\n",
      "\n",
      "str (question_number): 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "question: {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}\n",
      "\n",
      "\n",
      "str (question_number): 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_b\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "question: {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_c\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is a process by which computers analyze data and learn from it.'}, {'question_number': 2, 'answer': 'Overfitting is a significant issue in machine learning where a model performs well on training data but poorly on unseen data.'}, {'question_number': 3, 'answer': 'Supervised learning involves training a model on a labeled dataset.'}, {'question_number': 4, 'answer': 'Gradient descent is an iterative optimization algorithm used for minimizing a function.'}, {'question_number': 5, 'answer': 'AI encompasses a wide range of technologies, while ML specifically focuses on algorithms that improve with experience.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are crucial for neural networks as they introduce non-linearities into the model, enabling it to learn complex relationships in the data. Common activation functions include:\\n\\n1. **Sigmoid**: It squashes the output to be between 0 and 1, making it ideal for binary classification tasks. However, its major drawback is the vanishing gradient problem, where gradients become very small for large inputs, slowing down learning.\\n\\n2. **Tanh**: This function is similar to the sigmoid but outputs values between -1 and 1. It is generally preferred over sigmoid because it has a mean of zero, which helps in faster convergence during training.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function. It outputs the input directly if it is positive; otherwise, it outputs zero. This allows for faster training and helps to mitigate the vanishing gradient issue, making it a popular choice in deep networks.\\n\\n4. **Softmax**: This function converts raw scores (logits) from the last layer of the network into probabilities, ensuring that the total probability across all classes equals 1. It is particularly useful in multi-class classification problems.\\n\\nIn summary, the choice of activation function impacts the learning process and the performance of the neural network.'}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) have a wide range of applications, especially in the field of computer vision. They are designed to process data with a grid-like topology, such as images. Here are some prominent applications:\\n\\n1. **Image Classification**: CNNs can categorize images into different classes, such as distinguishing between dogs and cats based on learned features.\\n\\n2. **Object Detection**: They can identify and locate objects within images, a task crucial for autonomous vehicles and surveillance systems.\\n\\n3. **Image Segmentation**: This process involves dividing an image into segments for more granular analysis, such as identifying different regions in medical images.\\n\\n4. **Facial Recognition**: CNNs are widely used in security systems for recognizing individuals based on facial features, which has applications in both personal and public safety.\\n\\n5. **Medical Imaging**: They play a vital role in analyzing X-rays, MRIs, and CT scans to assist in diagnosing diseases, highlighting any abnormalities.\\n\\n6. **Video Analysis**: CNNs are used in analyzing video data for various applications, including action recognition in sports or identifying suspicious activities in surveillance footage.\\n\\nIn conclusion, CNNs have transformed how we process and analyze visual data, making them invaluable in many fields.'}], 'part_c': [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}]} \n",
      "\n",
      " Printing  part_c \n",
      "\n",
      " [{'question_number': 1, 'answer': \"Backpropagation is a key algorithm in training neural networks. It consists of two main phases: forward propagation and backward propagation. The forward pass involves sending the input data through the network to generate an output. Once the output is obtained, it is compared to the actual target output to compute the loss using a predefined loss function.\\n\\nIn the backward pass, the algorithm computes the gradient of the loss function with respect to each weight in the network. This is done using the chain rule of calculus, allowing the model to determine how much each weight contributed to the loss. The calculated gradients are then used to update the weights in the network through an optimization algorithm, typically gradient descent, where weights are adjusted in the direction that reduces the loss.\\n\\n**Example**: Consider a neural network designed to recognize handwritten digits. When an image of a handwritten digit is fed into the network, it goes through the forward pass, and the network outputs a prediction. If the prediction is incorrect, the loss is calculated, and during the backpropagation step, the algorithm computes the gradients based on this loss. The weights are then updated to improve the accuracy of the network's predictions over successive training iterations. Through many epochs of this process, the network learns to recognize handwritten digits accurately.\"}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S003.json\n",
      "Calculating total marks for student...\n",
      "Total marks: 0\n",
      "✌𝓟𝓻𝓸𝓬𝓮𝓼𝓼𝓲𝓷𝓰 𝓼𝓽𝓾𝓭𝓮𝓷𝓽✌: S004\n",
      " Inside process_student \n",
      "\n",
      "student = {'student_id': 'S004', 'name': 'David Lee', 'department': 'Software Engineering', 'answers': {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]}} \n",
      " scores =[]\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_a\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "question: {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "question: {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}\n",
      "\n",
      "\n",
      "str (question_number): 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "question: {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}\n",
      "\n",
      "\n",
      "str (question_number): 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "question: {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}\n",
      "\n",
      "\n",
      "str (question_number): 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_a \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_b\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "question: {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}\n",
      "\n",
      "\n",
      "str (question_number): 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_b \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "\n",
      "\\█▓▒▒░░░PART░░░▒▒▓█part_c\n",
      "\n",
      "                             questions: [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}] \n",
      "\n",
      "                                  from answer_key.items dict_items([('part_a', [{'question_number': 1, 'answer': 'Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. It focuses on the development of programs that can access data and use it to learn for themselves.'}, {'question_number': 2, 'answer': 'Overfitting in neural networks occurs when a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This leads to poor performance on new, unseen data. It can be mitigated through techniques such as regularization, dropout, and using more training data.'}, {'question_number': 3, 'answer': 'Supervised learning is a type of machine learning where a model is trained on labeled data, meaning the input data is paired with the correct output. The model learns to map inputs to outputs based on this training data, allowing it to make predictions on new, unseen data.'}, {'question_number': 4, 'answer': 'Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models, particularly neural networks. It involves calculating the gradient (the slope) of the loss function with respect to the model parameters and adjusting the parameters in the opposite direction of the gradient to reduce the loss.'}, {'question_number': 5, 'answer': 'Artificial Intelligence (AI) is a broader concept that encompasses the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence. Machine Learning (ML), on the other hand, is a subset of AI that focuses specifically on the development of algorithms that allow computers to learn from and make predictions based on data.'}]), ('part_b', [{'question_number': 1, 'answer': \"Activation functions are mathematical functions that determine the output of a neural network node given an input or set of inputs. There are several types of activation functions, including:\\n\\n1. **Sigmoid Function**: Produces an output between 0 and 1, making it useful for binary classification tasks. However, it can cause vanishing gradient problems.\\n\\n2. **Tanh Function**: Similar to the sigmoid but outputs values between -1 and 1, helping to center the data. It also suffers from the vanishing gradient issue.\\n\\n3. **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. This helps to mitigate the vanishing gradient problem and is widely used in hidden layers of deep networks.\\n\\n4. **Softmax Function**: Converts a vector of values into probabilities, useful for multi-class classification tasks by ensuring that the total sum of the outputs equals 1.\\n\\nEach activation function has its strengths and weaknesses, and the choice of which to use can significantly impact the model's performance.\"}, {'question_number': 2, 'answer': 'Convolutional Neural Networks (CNNs) are particularly effective for tasks involving image processing and computer vision. Their applications include:\\n\\n1. **Image Classification**: Assigning labels to images based on their content, commonly used in applications like facial recognition and object detection.\\n\\n2. **Image Segmentation**: Dividing an image into segments for easier analysis, such as identifying different objects within an image or separating foreground from background.\\n\\n3. **Object Detection**: Identifying and localizing objects within images, useful in autonomous driving and surveillance systems.\\n\\n4. **Medical Image Analysis**: Assisting in diagnosing diseases from medical images like X-rays or MRIs by detecting abnormalities.\\n\\n5. **Video Analysis**: Analyzing video data for applications such as activity recognition and motion detection.\\n\\n6. **Image Generation**: Techniques like Generative Adversarial Networks (GANs) use CNNs for generating realistic images from noise.\\n\\nCNNs leverage the spatial structure of images, allowing them to achieve high levels of accuracy in these applications.'}]), ('part_c', [{'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}])]) \n",
      "\n",
      "\n",
      "question: {'question_number': 1, 'answer': 'Backpropagation is a supervised learning algorithm used for training neural networks. It involves the following steps:\\n\\n1. **Forward Pass**: The input data is passed through the network layer by layer, producing an output. The output is compared to the actual target output to calculate the loss using a loss function.\\n\\n2. **Calculating Gradients**: The loss is then propagated back through the network to compute the gradients of the loss with respect to each weight in the network using the chain rule of calculus. This process determines how much each weight contributed to the error.\\n\\n3. **Weight Update**: Using the computed gradients, the weights of the network are updated to minimize the loss. This is typically done using gradient descent or its variants, adjusting the weights in the opposite direction of the gradient.\\n\\n**Example**: Consider a simple neural network with one input layer, one hidden layer, and one output layer. When a training example is fed into the network:\\n- The input values are transformed through the weights and activation functions, producing an output.\\n- If the output differs from the target, the loss is calculated.\\n- During backpropagation, the gradients of the loss are computed with respect to the weights in the hidden and input layers. This allows for fine-tuning of the weights in the next iteration of training, enabling the network to improve its predictions over time.'}\n",
      "\n",
      "\n",
      "str (question_number): 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "getting ans from  {'part_a': [{'question_number': 1, 'answer': 'Machine learning is when computers are trained to learn from data.'}, {'question_number': 2, 'answer': 'Overfitting is a problem where the model learns the training data too well.'}, {'question_number': 3, 'answer': 'Supervised learning is when a model is trained on labeled data.'}, {'question_number': 4, 'answer': 'Gradient descent is a method to reduce error in models.'}, {'question_number': 5, 'answer': 'AI is the concept while ML is a method of achieving AI.'}], 'part_b': [{'question_number': 1, 'answer': 'Activation functions are an essential component in neural networks that determine how signals are transformed and passed through the layers. Different types of activation functions are designed to handle various tasks, enabling the network to learn complex patterns in data.\\n\\n1. **Sigmoid**: This function outputs a value between 0 and 1, making it suitable for binary classification tasks. However, it suffers from the vanishing gradient problem, where gradients can become very small, slowing down learning.\\n\\n2. **Tanh**: Similar to the sigmoid function, tanh outputs values between -1 and 1, helping to center the data and often resulting in faster convergence compared to sigmoid.\\n\\n3. **ReLU (Rectified Linear Unit)**: ReLU has gained popularity in deep learning because it outputs the input directly if it is positive and zero otherwise. This alleviates the vanishing gradient issue and accelerates convergence, allowing networks to learn efficiently.\\n\\n4. **Softmax**: Used in multi-class classification tasks, the softmax function converts logits into probabilities, ensuring that the total probability across all classes equals 1.\\n\\nIn summary, the choice of activation function can significantly impact the performance and convergence speed of a neural network.'}, {'question_number': 2, 'answer': 'CNNs are powerful tools in the field of image processing. They can be applied in various domains, and their effectiveness lies in their ability to automatically learn features from data. Some of the main applications include:\\n\\n1. **Image Classification**: CNNs are used to classify images based on their content. For instance, they can distinguish between various objects, such as identifying animals or objects in photographs.\\n\\n2. **Object Detection**: These networks can locate and identify objects within an image. This capability is essential in applications such as autonomous driving, where recognizing pedestrians and other vehicles is critical.\\n\\n3. **Image Segmentation**: CNNs can divide images into segments to analyze different regions independently. This is particularly useful in medical imaging, where identifying specific areas can aid in diagnosis.\\n\\n4. **Facial Recognition**: CNNs are employed in security systems for identifying individuals based on their facial features, which is widely used in surveillance and personal devices.\\n\\n5. **Medical Imaging**: CNNs help analyze medical images, assisting in diagnosing diseases by highlighting anomalies or areas of concern.\\n\\nIn conclusion, the applications of CNNs are extensive, and they have significantly advanced the field of computer vision.'}], 'part_c': [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}]} \n",
      "\n",
      " Printing  part_c \n",
      "\n",
      " [{'question_number': 1, 'answer': 'Backpropagation is a critical algorithm in training neural networks, consisting of two primary stages: forward and backward propagation. In the forward pass, input data is fed through the network, generating an output that is compared against the true target to compute the loss.\\n\\nIn the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight in the network using the chain rule of calculus. This step determines how much each weight contributed to the error. The calculated gradients are then used to update the weights in the network, typically employing an optimization technique like gradient descent to minimize the loss.\\n\\n**Example**: Consider a neural network designed to predict housing prices based on various input features. During training, input data representing different houses is processed through the network to produce predicted prices. After calculating the loss by comparing predicted prices to actual prices, backpropagation is used to compute gradients and update the model weights. Through repeated iterations of this process, the network learns to make accurate predictions about housing prices.'}] \n",
      "\n",
      "str (answer): \n",
      "Saved output to intermediate_scores_S004.json\n",
      "Calculating total marks for student...\n",
      "Total marks: 0\n",
      "Final results DataFrame created.\n"
     ]
    }
   ],
   "source": [
    "final_results = calculate_results(students, answer_key, save_intermediate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['student_id', 'name', 'department', 'answers'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
